{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75aab7a",
   "metadata": {},
   "source": [
    "https://edition.cnn.com/\n",
    "\n",
    "\n",
    "https://www.cnbc.com/weather-and-natural-disasters/\n",
    "https://www.euronews.com/tag/natural-disaster\n",
    "\n",
    "https://www.goodnewsnetwork.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ed0f5",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Check which news provider is more neutral, negative and Positive</li>\n",
    "\n",
    "</ul>\n",
    "<h4>Vader</h4>\n",
    "<p>Limits to consider neutral positive and negative</p>\n",
    "<h4>TextBlob</h4>\n",
    "\n",
    "\n",
    "<h4>Flair</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65050e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/salvado/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver   # for webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # for implicit and explict waits\n",
    "from selenium.webdriver.chrome.options import Options  # for suppressing the browser\n",
    "\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import unicodedata\n",
    "import contractions\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#regular expressions\n",
    "import re\n",
    "\n",
    "# pretrained (sentiment classification)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# textBlob \n",
    "from textblob import TextBlob\n",
    "\n",
    "# flair\n",
    "import flair\n",
    "\n",
    "# loop status viewer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# tensorflow libraries to train new classification model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# parallel computing\n",
    "from joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc98d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the driver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "789d98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodnews_scrap(web='https://www.goodnewsnetwork.org/', pages=1)-> list:\n",
    "    driver.get(web)\n",
    "    \n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 2\n",
    "    article_goodnews = []\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(3)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        #if (screen_height) * i > scroll_height:\n",
    "        #    break\n",
    "        print(i)\n",
    "        \n",
    "        soup_goodnews = BeautifulSoup(driver.page_source)\n",
    "        \n",
    "        news_list = soup_goodnews.find_all('div', class_='td-block-row')\n",
    "        for news in news_list:\n",
    "            for header in news.find_all('h3'):\n",
    "                if len(header.get_text().split()) > 0:\n",
    "                    article_goodnews.append(header.get_text())\n",
    "                    \n",
    "    return article_goodnews\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cda481e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodnews_scrap_v2(web='https://www.goodnewsnetwork.org/', n_articles = 10):\n",
    "    \"\"\"\n",
    "        This function is to perform webscrap from the website Goodnews. This website is a website that\n",
    "        will load more articles when one scrolldown. So we'll use Selenium to help us with that.\n",
    "        We define also the number of articles we want to scrap.\n",
    "        \n",
    "    :param web: Goodnews WebSite \n",
    "    :param n_articles:  number of articles to scrap from the website  \n",
    "    :return: list of headers from each article-\n",
    "    \"\"\"\n",
    "    \n",
    "    driver.get(web)\n",
    "    # loading time issues:\n",
    "    scroll_pause_time = 2 # seconds\n",
    "    screen_height = driver.execute_script('return window.screen.height;')  # get the height of one screen\n",
    "    scrolling = True\n",
    "    # for scrolling purposes\n",
    "    scroll_init = 0\n",
    "    i=0\n",
    "    \n",
    "    # init list article\n",
    "    article_title = []\n",
    "    \n",
    "    while scrolling:\n",
    "        driver.execute_script('window.scrollTo({scroll_init}, {screen_height});'.format(scroll_init=scroll_init, screen_height=screen_height))\n",
    "        i += 1\n",
    "        scroll_init = screen_height\n",
    "        screen_height = scroll_init + screen_height\n",
    "        time.sleep(3)\n",
    "        \n",
    "        soup_goodnews = BeautifulSoup(driver.page_source)\n",
    "        news_list = soup_goodnews.find_all('div', class_='td-block-row')\n",
    "        for news in news_list:\n",
    "            for header in news.find_all('h3'):\n",
    "                if len(header.get_text().split()) > 0:\n",
    "                    article_title.append(header.get_text())\n",
    "                    if len(article_title) > n_articles:\n",
    "                        scrolling = False\n",
    "                        break\n",
    "                        \n",
    "    return article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b40c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodnews Number of Articles: 324\n"
     ]
    }
   ],
   "source": [
    "articles_goodnews = goodnews_scrap_v2(n_articles=300)\n",
    "#articles_cnn = cnn_scrap()\n",
    "\n",
    "print(f'Goodnews Number of Articles: {len(articles_goodnews)}')\n",
    "#print(f'CNN Number of Articles: {len(articles_cnn)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "808fb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_scrap(web='https://edition.cnn.com/'):\n",
    "    driver.get(web)\n",
    "    soup_cnn = BeautifulSoup(driver.page_source)\n",
    "    article_title = []\n",
    "    for section in soup_cnn.find_all('section')[1:]:\n",
    "        if len(section.find_all('ul')) > 0:\n",
    "            for ul_elem in section.find_all('ul'):\n",
    "                for elem in ul_elem.find_all('li'):\n",
    "                    if len(elem.get_text().split()) > 0:\n",
    "                        article_title.append(elem.get_text())\n",
    "    return article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d827108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Number of Articles: 0\n"
     ]
    }
   ],
   "source": [
    "#articles_goodnews = goodnews_scrap()\n",
    "articles_cnn = cnn_scrap()\n",
    "\n",
    "#print(f'Goodnews Number of Articles: {len(articles_goodnews)}')\n",
    "print(f'CNN Number of Articles: {len(articles_cnn)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9ad7b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_scrapv2(web='https://edition.cnn.com/world'):\n",
    "    \"\"\"\n",
    "        This function is to perform webscrap from the website CNN. This function is going to scrap each \"layer\",\n",
    "        from the nav-bar on the top and the page \"in it\". (Africa, Europe, Americas, etc.)\n",
    "        \n",
    "    :param web: CNN World WebSite \n",
    "    :return: list of headers from each article-\n",
    "    \"\"\"\n",
    "    driver.get(web)\n",
    "    cnn_world = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    nav_items = cnn_world.find_all('div', class_='header__nav-item')\n",
    "    #print(nav_items)\n",
    "    url_list = []\n",
    "    for url in nav_items:\n",
    "        url_list.append(url.find('a')['data-zjs-destination_url'])\n",
    "    article_title = []    \n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        page_soup = BeautifulSoup(driver.page_source)\n",
    "        tmp = page_soup.find_all('section', class_='layout__wrapper')[0]\n",
    "        for header in tmp.find_all('div', class_='container__headline'):\n",
    "            if len(header.get_text().split()) > 0:\n",
    "                article_title.append(header.get_text(strip=True))\n",
    "\n",
    "    return article_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8407e50",
   "metadata": {},
   "source": [
    "<h3>Text preprocessing:</h3>\n",
    "<ul>\n",
    "    <li>remove accented characters from strings use UniDecode;  <b>check :D</b></li>\n",
    "    <li>remove punctuation;  <b>check :D</b></li>\n",
    "    <li>make everyword lowercase;  <b>check :D</b></li>\n",
    "    <li>if present: remove the \\n \\t, etc from the strings;  <b>check :D</b></li>\n",
    "    <li>try remove stopwords;  <b>check :D</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805ed6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a2e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text: str, stop_word=True)-> str:\n",
    "    \"\"\"\n",
    "    Function to preprocess text.\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    for word in text.split():\n",
    "        if word in list(contractions.contractions_dict.keys()):\n",
    "            tmp.append(contractions.contractions_dict[word])\n",
    "        elif re.findall(r\"[A-Za-z]+\"r\"'\"+r\"[A-Za-z]\", word):\n",
    "            split_at = re.search(r\"[']\", word).span()[0]\n",
    "            word = word[0:split_at]\n",
    "            tmp.append(word)\n",
    "        elif re.findall(r\"[a-zA-z0-9]+\"r\"-\"+r\"[a-zA-z0-9]\", word):\n",
    "            l =re.split(r'-', word)\n",
    "            tmp.extend(l)\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "    text = ' '.join(tmp)\n",
    "    text = text.translate(str.maketrans('','', punctuation))  # remove punctutations\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    if stop_word:\n",
    "        text = [word for word in text.split() if word not in STOPWORDS]\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f19a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_vader(articles: list, dataframe_is_or_no=True)-> pd.DataFrame:\n",
    "    DS = SentimentIntensityAnalyzer()\n",
    "    articles_ = []\n",
    "    article_class = []\n",
    "    for article in articles:\n",
    "        text_original_classified = DS.polarity_scores(article)\n",
    "        text_processed = text_preprocessing(article, False)\n",
    "        text_processed_classified = DS.polarity_scores(text_processed)\n",
    "        if dataframe_is_or_no:\n",
    "            articles_.append([article, text_original_classified['compound'], text_processed, text_processed_classified['compound']])\n",
    "        else:\n",
    "            article_class.append([text_original_classified['compound'], text_processed_classified['compound']])\n",
    "    if dataframe_is_or_no:        \n",
    "        return pd.DataFrame(articles_, columns=['Original', 'Original Sent', 'Processed', 'Processed Sent'])\n",
    "    else:\n",
    "        return article_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f07d74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_Texblob(articles: list, dataframe_is_or_no=True)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that will classify a sentence.\n",
    "    \n",
    "    \"\"\"\n",
    "    articles_ = []\n",
    "    article_class = []\n",
    "    for article in articles:\n",
    "        text_original_classified = TextBlob(article).sentiment\n",
    "        text_processed = text_preprocessing(article, False)\n",
    "        text_processed_classified = TextBlob(text_processed).sentiment\n",
    "        if dataframe_is_or_no:\n",
    "            articles_.append([article, text_original_classified, text_processed, text_processed_classified])\n",
    "        else:\n",
    "            article_class.append([text_original_classified[0], text_processed_classified[0]])\n",
    "        \n",
    "    if dataframe_is_or_no:        \n",
    "        return pd.DataFrame(articles_, columns=['Original', 'Original Sent', 'Processed', 'Processed Sent'])\n",
    "    else:\n",
    "        return article_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3720a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify_text_flair(articles: list, dataframe_is_or_no=True)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that will classify a sentence.\n",
    "    \n",
    "    \"\"\"\n",
    "    articles_ = []\n",
    "    article_class = []\n",
    "    flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
    "    for article in tqdm(articles):\n",
    "        text_original_classified = flair.data.Sentence(article)\n",
    "        flair_sentiment.predict(text_original_classified)\n",
    "        sentiment_o = text_original_classified.score\n",
    "        if re.findall('negative', str(text_original_classified.labels[0]), re.IGNORECASE):\n",
    "            # print('not cool')\n",
    "            sentiment_o = -1 * float(sentiment_o)\n",
    "        \n",
    "        \n",
    "        \n",
    "        text_processed = text_preprocessing(article, False)\n",
    "        text_processed_classified = flair.data.Sentence(text_processed)\n",
    "        flair_sentiment.predict(text_processed_classified)\n",
    "        sentiment_p = text_processed_classified.score\n",
    "        if re.findall('negative', str(text_processed_classified.labels[0]), re.IGNORECASE):\n",
    "            # print('not cool')\n",
    "            sentiment_p = -1 * float(sentiment_p)\n",
    "        \n",
    "        \n",
    "        if dataframe_is_or_no:\n",
    "            articles_.append([article, sentiment_o, text_processed, sentiment_p])\n",
    "        else:\n",
    "            article_class.append([sentiment_o, sentiment_p])\n",
    "        \n",
    "    if dataframe_is_or_no:        \n",
    "        return pd.DataFrame(articles_, columns=['Original', 'Original Sent', 'Processed', 'Processed Sent'])\n",
    "    else:\n",
    "        return article_class  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b46b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 11.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11, 4), (11, 4), (11, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_df = classify_text_vader(articles_goodnews)\n",
    "textblob_df = classify_text_Texblob(articles_goodnews)\n",
    "flair_df = clasify_text_flair(articles_goodnews)\n",
    "vader_df.shape, textblob_df.shape, flair_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24eb11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'Original':vader_df['Original'],\n",
    "          'O_Vader': vader_df['Original Sent'],\n",
    "          'O_TextBlob': textblob_df['Original Sent'],\n",
    "          'O_Flair': flair_df['Original Sent'],\n",
    "          'Processed':vader_df['Processed'], \n",
    "          'P_Vader':vader_df['Processed Sent'],\n",
    "          'P_TextBlob': textblob_df['Processed Sent'],\n",
    "          'P_Flair': flair_df['Processed Sent']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72e08970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>O_Vader</th>\n",
       "      <th>O_TextBlob</th>\n",
       "      <th>O_Flair</th>\n",
       "      <th>Processed</th>\n",
       "      <th>P_Vader</th>\n",
       "      <th>P_TextBlob</th>\n",
       "      <th>P_Flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good News in History, February 21</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>(0.7, 0.6000000000000001)</td>\n",
       "      <td>0.990672</td>\n",
       "      <td>good news in history february 21</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>(0.7, 0.6000000000000001)</td>\n",
       "      <td>0.996886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First of Its Kind Discovery in Mali: Vast Rese...</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>(0.3041666666666667, 0.7333333333333334)</td>\n",
       "      <td>0.996523</td>\n",
       "      <td>first of its kind discovery in mali vast reser...</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>(0.3041666666666667, 0.7333333333333334)</td>\n",
       "      <td>0.997266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing Footage Shows Birth of ‘Precious’ Rare...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>(0.46666666666666673, 0.9333333333333332)</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>amazing footage shows birth of precious rare t...</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>(0.46666666666666673, 0.9333333333333332)</td>\n",
       "      <td>0.999780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Astronomers Observe 2 Neutron Stars Colliding ...</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>(-0.125, 1.0)</td>\n",
       "      <td>0.867790</td>\n",
       "      <td>astronomers observe 2 neutron stars colliding ...</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>(-0.125, 1.0)</td>\n",
       "      <td>0.945756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>North America’s Only Native Stork Poised to Fl...</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>(0.4, 0.95)</td>\n",
       "      <td>-0.951812</td>\n",
       "      <td>north americas only native stork poised to fly...</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>(0.4, 0.95)</td>\n",
       "      <td>-0.835691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  O_Vader  \\\n",
       "0                  Good News in History, February 21   0.4404   \n",
       "1  First of Its Kind Discovery in Mali: Vast Rese...   0.7269   \n",
       "2  Amazing Footage Shows Birth of ‘Precious’ Rare...   0.5859   \n",
       "3  Astronomers Observe 2 Neutron Stars Colliding ...  -0.1280   \n",
       "4  North America’s Only Native Stork Poised to Fl...   0.2500   \n",
       "\n",
       "                                  O_TextBlob   O_Flair  \\\n",
       "0                  (0.7, 0.6000000000000001)  0.990672   \n",
       "1   (0.3041666666666667, 0.7333333333333334)  0.996523   \n",
       "2  (0.46666666666666673, 0.9333333333333332)  0.999854   \n",
       "3                              (-0.125, 1.0)  0.867790   \n",
       "4                                (0.4, 0.95) -0.951812   \n",
       "\n",
       "                                           Processed  P_Vader  \\\n",
       "0                   good news in history february 21   0.4404   \n",
       "1  first of its kind discovery in mali vast reser...   0.7269   \n",
       "2  amazing footage shows birth of precious rare t...   0.8176   \n",
       "3  astronomers observe 2 neutron stars colliding ...  -0.1280   \n",
       "4  north americas only native stork poised to fly...   0.2500   \n",
       "\n",
       "                                  P_TextBlob   P_Flair  \n",
       "0                  (0.7, 0.6000000000000001)  0.996886  \n",
       "1   (0.3041666666666667, 0.7333333333333334)  0.997266  \n",
       "2  (0.46666666666666673, 0.9333333333333332)  0.999780  \n",
       "3                              (-0.125, 1.0)  0.945756  \n",
       "4                                (0.4, 0.95) -0.835691  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f292146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 39/39 [00:03<00:00, 11.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# cnn news\n",
    "vader_df = classify_text_vader(articles_cnn)\n",
    "textblob_df = classify_text_Texblob(articles_cnn)\n",
    "flair_df = clasify_text_flair(articles_cnn)\n",
    "# vader_df.shape, textblob_df.shape, flair_df.shape\n",
    "\n",
    "df_dict = {'Original':vader_df['Original'],\n",
    "          'O_Vader': vader_df['Original Sent'],\n",
    "          'O_TextBlob': textblob_df['Original Sent'],\n",
    "          'O_Flair': flair_df['Original Sent'],\n",
    "          'Processed':vader_df['Processed'], \n",
    "          'P_Vader':vader_df['Processed Sent'],\n",
    "          'P_TextBlob': textblob_df['Processed Sent'],\n",
    "          'P_Flair': flair_df['Processed Sent']}\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dce24f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Original      Director on making beloved children's book cha...\n",
       "O_Vader                                                  0.5106\n",
       "O_TextBlob                          (0.09999999999999998, 0.95)\n",
       "O_Flair                                               -0.976635\n",
       "Processed     director making beloved children book characte...\n",
       "P_Vader                                                  0.5106\n",
       "P_TextBlob                          (0.09999999999999998, 0.95)\n",
       "P_Flair                                                0.937315\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3846457f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_cnn[14].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b19642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence[11]: \"Director on making beloved children's book character into bloodthirsty monster\"'/'NEGATIVE' (0.9766)]\n",
      "0.9766347408294678\n"
     ]
    }
   ],
   "source": [
    "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
    "\n",
    "text_original_classified = flair.data.Sentence(df['Original'][20])\n",
    "flair_sentiment.predict(text_original_classified)\n",
    "sentiment_o = text_original_classified.labels\n",
    "print(sentiment_o)\n",
    "\n",
    "print(text_original_classified.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a8886673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[3\n"
     ]
    }
   ],
   "source": [
    "print(str(text_original_classified.labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "21bd2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixe\n"
     ]
    }
   ],
   "source": [
    "if re.findall('negative', str(text_original_classified.labels[0]), re.IGNORECASE):\n",
    "    print('not cool')\n",
    "else:\n",
    "    print('fixe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
